{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57773dc3-2276-4711-bd02-476ae34d921c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Binomial with Beta Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8f8ee-2ea8-4f6f-9b92-232e2ba60c95",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "\n",
    "Before we dive into the mathematics, let us consider a motivating example:\n",
    "\n",
    "---\n",
    "\n",
    "One day while walking downtown, you stop briefly to watch a street-performer - a magician - who is doing various sleight-of-hand tricks. As a part of his next trick, he shows the crowd a standard-appearing coin with both head and tails sides. Somewhat abruptly the magician singles you out from the crowd and offers you a deal: if he flips the coin 10 times and it lands on heads 7 or less times, he will pay you 100 dollars, but if it lands on heads 8 or more times, you owe him 100 dollars. The question: do you take the bet?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737effff-3695-4491-b54f-8f673463b1b7",
   "metadata": {},
   "source": [
    "## Expected Value of A Fair Coin\n",
    "\n",
    "Let us first calculate the expected value of the above proposition assuming the coin is fair.\n",
    "\n",
    "To begin we model the coin as a random variable over $n$ Bernoulli trials with $x$ number of heads and $n-x$ tails according to some unknown probability $\\theta$ of heads and a complementary probability $(1-\\theta)$ of tails.\n",
    "\n",
    "The random variable will follow the binomial distribution with a probability mass function (pmf) of $$p(n,x|\\theta)={n \\choose x}\\theta^{x}(1-\\theta)^{n-x}$$\n",
    "\n",
    "We begin by graphing the probability of each combination of heads and tails and coloring it by its associated betting outcome, in addition to calculate the expected value of the bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9486e804-f2d5-4141-864d-966b2be3cc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fp/txtrdyy53t32f1msdpsjcpdc0000gq/T/ipykernel_78999/3816346436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#plot the distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Win $100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Lose $100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Probability'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_values' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# set parameter values\n",
    "n = 10 # number of flips\n",
    "theta = 0.5 # probability of heads\n",
    "\n",
    "#define possible outcomes of heads\n",
    "x_values = np.arange(n+1)\n",
    "\n",
    "#calculate pmf values\n",
    "dist = np.array([binom.pmf(x,n,theta) for x in x_values])\n",
    "\n",
    "#create color masks\n",
    "win = x_values<=7\n",
    "lose = x_values>7\n",
    "\n",
    "#plot the distribution\n",
    "plt.bar(s_values[win],dist[win],color='green',label='Win $100')\n",
    "plt.bar(s_values[lose],dist[lose],color='red',label='Lose $100')\n",
    "plt.ylabel('Probability',fontsize='15')\n",
    "plt.xlabel('Number of Heads',fontsize='15')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# calculate expected value\n",
    "expected_value = 100*sum(dist[win]) - 100*sum(dist[lose])\n",
    "print(\"Expected Value:\",f\"${round(expected_value,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af891892-470a-463d-b469-198d627672b9",
   "metadata": {},
   "source": [
    "Given the above calculation, the proposition sounds like a wonderful deal with an expected value of $89.06 in your favor if the coin is fair; however, how do you know whether the coin is fair? Better yet, how would you quantify your uncertainty about the fairness of the coin? Suppose, in this hypothetical (although unlikely in reality) that the magician allows you to flip the coin a few times prior to taking the bet. How could you use the data that you collect to inform your decision by updating your uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e186e-af94-4c23-b29b-24491da94499",
   "metadata": {},
   "source": [
    "## Parameterizing a Possibly Unfair Coin\n",
    "\n",
    "As before, we model the coin as a random variable over $n$ Bernoulli trials with $x$ number of heads and $n-x$ number of tails according to some unknown probability $\\theta$ of heads and a complementary probability $(1-\\theta)$ of tails.\n",
    "\n",
    "The random variable will follow the binomial distribution with a probability mass function of \n",
    "\n",
    "$$p(n,x|\\theta)={n \\choose x}\\theta^{x}(1-\\theta)^{n-x}$$\n",
    "\n",
    "The conjugate prior $p(\\theta)$ of the binomial distribution is the beta distribution.\n",
    "\n",
    "$$p(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}$$\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are hyperparameters encoding prior belief or information and $B(\\alpha,\\beta)$ represents the Beta function serving as a normalizing constant.\n",
    "\n",
    "Plugging the above equations into Bayes' theorem, simplifying, and renormalizing, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta|n,x) & = \\frac{p(n,x|\\theta)p(\\theta)}{\\int p(n,x|\\theta)p(\\theta)d\\theta} \\\\\n",
    "& = \\frac{{n \\choose x}\\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}/B(\\alpha,\\beta)}{\\int_{0}^{1} {n \\choose x}\\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}/B(\\alpha,\\beta)d\\theta} \\\\\n",
    "& = \\frac{\\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}}{B(x+\\alpha,n-x+\\beta)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "which is a Beta distribution with parameters $(x+\\alpha,n-x+\\beta)$. The posterior distribution can then be used as a prior for further observations with the hyperparameters consolidating the information with each new addition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67356b04-cc33-4936-b6f2-c1ebfc79ecc3",
   "metadata": {},
   "source": [
    "## Intuition of the Beta Parameters\n",
    "\n",
    "The $\\alpha$ and $\\beta$ parameters of the Beta prior can seem somewhat cryptic at first, until one recognizes that they represent pseudo-observations. A Beta prior with parameters $(1,1)$ is uninformative as it represents a coin where one has witnessed no coin flips. Meanwhile a Beta prior of $(86,16)$ can be understood as a coin which we have flipped 100 times, with 85 heads and 15 tails. Thus in the process of updating our Beta prior using Bayes' theorem, it makes sense that the resulting posterior distribution would have parameters $(x+\\alpha,n-x+\\beta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdb2db-b0d1-4303-a9d9-f38a526524e2",
   "metadata": {},
   "source": [
    "## Estimating the Bias of a Coin\n",
    "\n",
    "Revisiting the motivating example, suppose you test the coin by flipping it 50 times and it lands on heads exactly 42 times and tails 8 times. Assuming an uninformative prior of $Beta(1,1)$, the posterior would have the form of $Beta(1+42,1+8)=Beta(43,9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4305d-e6c8-44ec-85fa-eae1b9eb48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "# set parameter values\n",
    "prior_alpha, prior_beta = 1,1\n",
    "posterior_alpha, posterior_beta = 43, 9\n",
    "\n",
    "# generate plot values\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "# generate distributions\n",
    "prior_dist = beta.pdf(x,prior_alpha,prior_beta)\n",
    "posterior_dist = beta.pdf(x,posterior_alpha,posterior_beta)\n",
    "\n",
    "# plot the prior and posterior distributions\n",
    "plt.plot(x,prior_dist,color='red',label=\"Prior: Beta(1,1)\")\n",
    "plt.plot(x,posterior_dist,color='blue',label=\"Posterior: Beta(43,9)\")\n",
    "plt.xlabel('Î¸',fontsize='15')\n",
    "plt.ylabel('Probability',fontsize='15')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b93e4-ac38-4542-8ad2-d631aa4fb8ec",
   "metadata": {},
   "source": [
    "From a single glance, we can see that the probability of Î¸ being 0.5 (i.e. the coin being fair) is vanishingly small, while most of the consolidation is occurring somewhere around 0.83.\n",
    "\n",
    "We can more specifically characterize the posterior distribution by calculating the maximum a posteriori (MAP) which is the global maximum of the posterior distribution. The MAP can provide a good point estimate for the value of theta in distributions that are unimodal and well-behaved.\n",
    "\n",
    "Further, we can calculate a credible interval for the posterior distribution. A credible interval is the interval over which an unobserved parameter (such as Î¸) falls with a particular probability. Although there are many ways to define credible intervals for different purposes, for this example we will use a 95% highest density interval, which is defined as the smallest interval on the posterior density which integrates to a given confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06230215-474b-4627-a930-5b71e6c439f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate maximum a posteriori\n",
    "p_max = max(posterior_dist)\n",
    "theta_max = x[np.argmax(posterior_dist)]\n",
    "\n",
    "#define function to calculate highest density credible interval\n",
    "from scipy import optimize\n",
    "\n",
    "def hdi(distribution, level=0.95):\n",
    "    \"\"\"\n",
    "    Get the highest density interval for the distribution, e.g. for a Bayesian posterior, the highest posterior density interval (HPD/HDI)\n",
    "    Credit: Ying Tong Li (https://yingtongli.me/blog/2022/10/06/scipy-hdi.html) \n",
    "    \"\"\"\n",
    "\n",
    "    # For a given lower limit, we can compute the corresponding 95% interval\n",
    "    def interval_width(lower):\n",
    "        upper = distribution.ppf(distribution.cdf(lower) + level)\n",
    "        return upper - lower\n",
    "\n",
    "    # Find such interval which has the smallest width\n",
    "    # Use equal-tailed interval as initial guess\n",
    "    initial_guess = distribution.ppf((1-level)/2)\n",
    "    optimize_result = optimize.minimize(interval_width, initial_guess,method = 'Nelder-Mead')\n",
    "\n",
    "    lower_limit = optimize_result.x[0]\n",
    "    width = optimize_result.fun\n",
    "    upper_limit = lower_limit + width\n",
    "\n",
    "    return (lower_limit, upper_limit)\n",
    "\n",
    "distribution = beta(posterior_alpha,posterior_beta)\n",
    "lower_bound, upper_bound = hdi(distribution,0.95)\n",
    "\n",
    "plt.plot(x,posterior_dist,color='blue',label=\"Posterior: Beta(43,9)\")\n",
    "plt.vlines(x=theta_max,ymin=0,ymax=p_max,color='purple',label=f\"MAP: Î¸={round(theta_max,2)}\")\n",
    "plt.fill_between(x,y1=posterior_dist,where=(lower_bound<x)&(x<upper_bound),color='blue',alpha=0.2,label=f\"95% HDI: [{round(lower_bound,2)},{round(upper_bound,2)}]\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e013a7-fd46-45ed-a490-3ee0c3d821a3",
   "metadata": {},
   "source": [
    "## Calculating the Expected Value for a Biased Coin\n",
    "\n",
    "The expected value of the biased coin can be calculated using the posterior predictive distribution.\n",
    "\n",
    "In Bayesian statistics, the posterior predictive distribution (PPD) is the distribution of possible unobserved values $\\tilde{x}$ conditioned on the set of $n$ previously observed values $\\textbf{X}=\\{x_1,...,x_n\\}$ which are drawn from a distribution that depends on parameter $\\theta\\in\\Theta$ where $\\Theta$ is the parameter space.\n",
    "\n",
    "The PPD is calculated by the marginalization of the distribution of $\\tilde{x}$ given $\\theta$ over the posterior distribution of $\\theta$ given $\\textbf{X}$.\n",
    "\n",
    "$$PPD\\stackrel{\\text{def}}{=} p(\\tilde{x}|\\textbf{X})=\\int_{\\theta}p(\\tilde{x}|\\theta)p(\\theta|\\textbf{X})d\\theta$$\n",
    "\n",
    "As we saw in the case of the unbiased coin, $p(\\tilde{x}|\\theta)$ follows a binomial distribution. Additionally, as we learned above, the conjugate posterior $p(\\theta|\\textbf{X})$ of the binomial distribution is the beta distribution. Thus,\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\tilde{x}|\\textbf{X}) & = \\int_{\\theta} p(\\tilde{x}|\\theta)p(\\theta|\\textbf{X})d\\theta \\\\\n",
    "& = \\int_0^1Bin(\\tilde{x}|n,\\theta)Beta(\\theta|\\alpha,\\beta)d\\theta \\\\\n",
    "& = {n \\choose x}\\frac{1}{B(\\alpha,\\beta)}\\int_0^1\\theta^{x+\\alpha-1}(1-\\theta)^{n-x+\\beta-1}d\\theta \\\\\n",
    "& = {n \\choose x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2030d-ef35-4daf-8ef9-5b0fc76e9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import betabinom\n",
    "\n",
    "# set parameter values\n",
    "n = 10 # number of flips\n",
    "alpha = 43 # number of observed successes + 1\n",
    "beta = 9 # number of observed failures + 1\n",
    "\n",
    "#define possible outcomes of success/heads\n",
    "x_values = np.arange(n+1)\n",
    "\n",
    "#calculate pmf values\n",
    "dist = np.array([betabinom.pmf(x,n,alpha,beta) for x in x_values])\n",
    "\n",
    "#create color masks\n",
    "win = s_values<=7\n",
    "lose = s_values>7\n",
    "\n",
    "#plot the distribution\n",
    "plt.bar(x_values[win],dist[win],color='green',label='Win $100')\n",
    "plt.bar(x_values[lose],dist[lose],color='red',label='Lose $100')\n",
    "plt.ylabel('Probability',fontsize='15')\n",
    "plt.xlabel('Number of Heads',fontsize='15')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# calculate expected value of the bet\n",
    "expected_value = 100*sum(dist[win]) - 100*sum(dist[lose])\n",
    "print(\"Expected Value:\",f\"{round(expected_value,2)}\",\"dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc87f7-b2cf-49ab-8f21-e636e9b21b9e",
   "metadata": {},
   "source": [
    "Thus, we would could expect to lose an average of $49.40 every time we make the bet with the magician, given the prior observations of 42 heads in 50 tosses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
